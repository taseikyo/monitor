# -*- coding: utf-8 -*-
# @Author: Lewis Tian
# @Date:   2025-05-13 31:39:09

import json
import os
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from logging import Logger
from typing import Dict, List
from urllib.parse import urlparse

import requests

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import bootstrap  # noqa: F401, E402
from model.pixiv_illustration import PixivItemUrlInfo  # noqa: E402

MAX_RETRIES = 3
CONCURRENT_LIMIT = 10
IMAGE_QUALITY = ["original", "regular", "small", "thumb_mini"]


class PixivImage:
    """
    PixivImageç±»ï¼Œç”¨äºè·å–Pixivå›¾ç‰‡çš„URLä¿¡æ¯
    """

    def __init__(self, logger: Logger, pid: int):
        self.logger = logger
        self.pid = pid

    def get_image_urls(self) -> List[str]:
        """
        è·å–å›¾ç‰‡æ‰€æœ‰çš„URLé“¾æ¥
        """
        headers = {
            "referer": "https://www.pixiv.net/ranking.php",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
        }
        url = f"https://www.pixiv.net/ajax/illust/{self.pid}/pages?lang=zh"

        try:
            response = requests.get(url, headers=headers, timeout=10)
            self.logger.info(f"Request URL: {response.url}")
            self.logger.info(f"Response Text: {response.text.replace('\n', '')}")
            resp = response.json()
        except requests.RequestException as e:
            self.logger.error(f"Request failed: {e}")
            return []
        except json.JSONDecodeError as e:
            self.logger.error(f"JSON decode failed: {e}")
            return []

        if not resp:
            self.logger.warning("Empty response.")
            return []

        data = resp.get("body", [])

        urls = []
        for pic in data:
            urls = pic.get("urls", {})
            for x in IMAGE_QUALITY:
                url = urls.get(x, "")
                if url:
                    urls.append(url)
                    break

        self.logger.info(f"pid: {self.pid}, urls: {urls}")
        return urls

    def get_image_info(self) -> PixivItemUrlInfo:
        """
        è·å–å›¾ç‰‡çš„ URL ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼šé“¾æ¥ï¼Œç‚¹èµæ•°ï¼Œè¯„è®ºæ•°ï¼Œæ”¶è—æ•°
        ç›¸æ¯”äº get_image_urls å¤šäº†å›¾ç‰‡çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œä½†æ˜¯ä»…æ‹¿åˆ°ç¬¬ä¸€å¼ å›¾ç‰‡çš„ url
        """
        headers = {
            "referer": "https://www.pixiv.net/ranking.php",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
        }
        url = f"https://www.pixiv.net/ajax/illust/{self.pid}?lang=zh"

        try:
            response = requests.get(url, headers=headers, timeout=10)
            self.logger.info(f"ğŸ” Request URL: {response.url}")
            self.logger.info(f"ğŸ“„ Response Text: {response.text}")
            resp = response.json()
        except requests.RequestException as e:
            self.logger.error(f"âŒ Request failed: {e}")
            return None
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ JSON decode failed: {e}")
            return None

        if not resp:
            self.logger.warning("âš ï¸  Empty response.")
            return None

        try:
            return PixivItemUrlInfo.model_validate(resp.get("body", {}))
        except Exception as e:
            self.logger.error(f"âŒ Failed to parse PixivItemUrlInfo: {e}")
            return None


def batch_get_image_infos(
    logger: Logger, pids: List[int], max_workers: int = 10
) -> Dict[int, PixivItemUrlInfo]:
    """
    æ‰¹é‡è·å–å›¾ç‰‡çš„urlä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼šé“¾æ¥ï¼Œç‚¹èµæ•°ï¼Œè¯„è®ºæ•°ï¼Œæ”¶è—æ•°
    :param logger: æ—¥å¿—è®°å½•å™¨
    :param pids: å›¾ç‰‡ ID åˆ—è¡¨
    :param max_workers: æœ€å¤§çº¿ç¨‹æ•°
    :return: å›¾ç‰‡ ID å’Œå¯¹åº”çš„ URL ä¿¡æ¯å­—å…¸
    """
    result = {}
    wroks = [PixivImage(logger, pid) for pid in pids]

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_pid = {
            executor.submit(work.get_image_info): work.pid for work in wroks
        }
        for future in as_completed(future_to_pid):
            pid = future_to_pid[future]
            try:
                result[pid] = future.result()
            except Exception as e:
                logger.error(f"âŒ Failed to get url for pid {pid}: {e}")
                result[pid] = None

    return result


def batch_get_image_urls(
    logger: Logger, pids: List[int], max_workers: int = 10
) -> Dict[int, List[str]]:
    """
    æ‰¹é‡è·å–å›¾ç‰‡çš„ URL
    :param logger: æ—¥å¿—è®°å½•å™¨
    :param pids: å›¾ç‰‡ ID åˆ—è¡¨
    :param max_workers: æœ€å¤§çº¿ç¨‹æ•°
    :return: å›¾ç‰‡ URL åˆ—è¡¨
    """
    result = {}
    wroks = [PixivImage(logger, pid) for pid in pids]

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_pid = {
            executor.submit(work.get_image_urls): work.pid for work in wroks
        }
        for future in as_completed(future_to_pid):
            pid = future_to_pid[future]
            try:
                urls = future.result()
                result[pid] = urls
            except Exception as e:
                logger.error(f"Failed to get urls for pid {pid}: {e}")
                result[pid] = []

    return result


def download_image_stream(logger: Logger, url: str, save_path: str) -> None:
    """
    ä¸‹è½½å›¾ç‰‡
    :param logger: æ—¥å¿—è®°å½•å™¨
    :param url: å›¾ç‰‡ URL
    :param save_path: ä¿å­˜è·¯å¾„
    """
    headers = {
        "referer": "https://www.pixiv.net/",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    }
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            response = requests.get(url, headers=headers, stream=True)
            if response.status_code != 200:
                logger.warning(f"âš ï¸ çŠ¶æ€ç  {response.status_code}ï¼Œç¬¬ {attempt} æ¬¡é‡è¯•: {url}")
                continue
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            with open(save_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            logger.info(f"âœ… ä¸‹è½½æˆåŠŸ: {os.path.basename(save_path)}")
            return

        except requests.RequestException as e:
            logger.error(f"è¯·æ±‚å¤±è´¥: {e}, å°è¯•é‡è¯•ç¬¬ {attempt} æ¬¡: {url}")

    logger.error(f"âŒ æœ€ç»ˆå¤±è´¥: {url}")


def batch_download_images(
    logger: Logger, urls: List[str], save_paths: List[str], max_workers: int = 10
) -> None:
    """
    æ‰¹é‡ä¸‹è½½å›¾ç‰‡
    :param logger: æ—¥å¿—è®°å½•å™¨
    :param urls: å›¾ç‰‡ URL åˆ—è¡¨
    :param save_paths: ä¿å­˜è·¯å¾„åˆ—è¡¨
    :param max_workers: æœ€å¤§çº¿ç¨‹æ•°
    """
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            executor.submit(download_image_stream, logger, url, save_path)
            for url, save_path in zip(urls, save_paths)
        ]
        for future in futures:
            future.result()


def get_url_basename(url: str) -> str:
    parsed_url = urlparse(url)
    basename = os.path.basename(parsed_url.path)
    return basename


def filter_and_save_image_by_map(
    logger: Logger,
    user_id: str,
    basename: str,
    global_map: Dict[str, List[str]],
    local_map: Dict[str, List[str]],
) -> bool:
    """
    æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å·²ç»å­˜åœ¨äºå…¨å±€æˆ–æœ¬åœ°æ˜ å°„ä¸­
    :param logger: æ—¥å¿—è®°å½•å™¨
    :param user_id: ç”¨æˆ· ID
    :param basename: å›¾ç‰‡æ–‡ä»¶å
    :param global_map: å…¨å±€æ˜ å°„
    :param local_map: æœ¬åœ°æ˜ å°„
    :return: å¦‚æœå›¾ç‰‡å·²ç»å­˜åœ¨äºå…¨å±€æˆ–æœ¬åœ°æ˜ å°„ä¸­ï¼Œåˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    if basename in global_map.get(user_id, []):
        logger.info(f"ğŸ“‚ Exists in global, skip: {basename}")
        return True
    if basename in local_map.get(user_id, []):
        logger.info(f"ğŸ“‚ Exists in local, skip: {basename}")
        return True
    else:
        local_map[user_id] = []
    local_map[user_id].append(basename)

    return False
